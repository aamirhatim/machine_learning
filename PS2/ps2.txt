Aamir Husain
EECS 349: Machine Learning
Problem Set 2


1.
The "alcohol" attribute looks to be the most useful as it seems to separate the good and bad wines along either end of the spectrum compared to the other histograms. Most other attributes have fairly similar distributions, but the "alcohol" attribute shows a higher concentration of "bad" wine on the left and more "good" wines on the right.

2.
ZeroR has an accuracy of 62.381%. ZeroR simply predicts the majority class in a set of data and ignores any other attributes. This is useful for evaluating other classifiers because you can see its actual performance gain compared to just doing a majority vote. For example, if ZeroR output an accuracy of 95% and your own classifier had an accuracy of 95.5%, the classifier probably isn't very useful since there is not much improvement from ZeroR.

3.
Looking at the output tree, "alcohol" is the most informative split, which is the same attribute predicted in question 1.

4.
Accuracy using "Use training set" option: 95.873%
Accuracy using "Cross-validation" option with 10 folds: 85.9788%
Cross-validation essentially bins the entire data set into an x number of folds. One fold is used for validation and the rest are used for training. A 10-fold cross-validation method splits the data into ten bins, uses 9 of them for training, and then uses the last one for validation. This process is repeated so that each bin is eventually used as a validation set and the accuracy from each trial is compared and averaged to produce the total accuracy of the classifier.
The main reason the accuracy of this method is lower than the accuracy of using the whole training set is because we simply have less data to make a more robust classifier. The classifier may also be overfitting the data since it is using the whole dataset as the training set.
This is very useful to indicate how well the classifier will perform on new data since cross-validation can simulate it by using different training and validation sets.

5.
The Random Forest classifier seems to give the best performance with and accuracy of 90.582% using 10-fold cross validation. The command line entry is <RandomForest -P 100 -I 100 -num-slots 1 -K 0 -M 1.0 -V 0.001 -S 1 -B>

6.
I was not familiar with most of the strategies mentioned, so I used a brute force method of trial and error to choose the best classifier. I ran 10-fold cross validation as well as varying percentage-split tests to compare the accuracy on the validation set. After the best result was found, I then changed the parameters within the classifier to see if I could further improve its accuracy.

7.
The two classifiers used were AdaBoostM1 <AdaBoostM1 -P 100 -S 1 -I 10 -W weka.classifiers.trees.DecisionStump> and RandomForest <RandomForest -P 100 -I 100 -num-slots 1 -K 0 -M 1.0 -V 0.001 -S 1 -B>. The results for 10-fold cross validation are as follows:
AdaBoostM1(wine)    = 81.640%
AdaBoostM1(car)     = 70.504%
RandomForest(wine)  = 90.582%
RandomForest(car)   = 93.193%

Using the formula in the homework: 81.640 + 93.193 - 90.582 - 70.504 = 13.747
These classifiers were chosen using the same approach as in question 6.

8.
f1(x): 1-NN. Because there seems to be distinct areas of positive and negative, using 1-NN would be good for this dataset. Using more neighbors may lead to generalizing new data points to a potentially wrong classification.
f2(x): Linear. The outputs follow a fairly linear path, with a little bit of noise for a couple of the data points.
f3(x): Polynomial. The outputs of this function could be a noisy set of linear relationships, but assuming the data is fairly clean, a polynomial classifier seems to suit this data better and yield a lower sum square error.
f4(x): 3-NN. The negative output of f4(4) in this data set looks like it is noise or bad data, mainly because the outputs for all other inputs are positive. For that reason 3-NN makes sense to use since using 1-NN may give more weight to noisy data.
